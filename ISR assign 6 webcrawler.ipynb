{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ef1d5f-07c8-48c5-a0e3-dd55e3943e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fb6d95-1f76-4e08-a580-51de3e30fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0edfb8-c97a-419c-907e-a6cda477ef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many pages to crawl:  5\n"
     ]
    }
   ],
   "source": [
    "pages = int(input(\"How many pages to crawl: \"))  # User input\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e3f080-97c3-4ba3-b890-f80a6f1efd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Page 1 done\n",
      "‚úÖ Page 2 done\n",
      "‚úÖ Page 3 done\n",
      "‚úÖ Page 4 done\n",
      "‚úÖ Page 5 done\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, pages + 1):\n",
    "    url = f\"https://books.toscrape.com/catalogue/page-{i}.html\"  # Page URL\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)  # Fetch page\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Page {i} failed, skipping\")  # Handle network issues\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")  # Parse HTML\n",
    "    for p in soup.select(\".product_pod\"):\n",
    "        title = p.h3.a[\"title\"].strip()  # Book title\n",
    "        raw_price = p.select_one(\".price_color\").text.strip()  # Raw price\n",
    "        price = raw_price.encode(\"latin1\").decode(\"utf-8\")  # Fix encoding (removes √Ç)\n",
    "        link = \"https://books.toscrape.com/catalogue/\" + p.h3.a[\"href\"]  # Full product link\n",
    "        data.append({\"Name\": title, \"Price\": price, \"Link\": link})  # Add data to list\n",
    "    print(f\"‚úÖ Page {i} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459f0d74-a708-428d-8635-1d1dcbcba7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Done! Saved 100 books to books.csv\n"
     ]
    }
   ],
   "source": [
    "with open(\"books.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"Name\", \"Price\", \"Link\"])  # Define headers\n",
    "    writer.writeheader()  # ‚úÖ Add headings\n",
    "    writer.writerows(data)  # Write all rows\n",
    "\n",
    "print(f\"\\nüéâ Done! Saved {len(data)} books to books.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2770758-8580-4070-add0-fff150cc9128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
